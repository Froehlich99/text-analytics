{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "080be2e1",
      "metadata": {},
      "source": [
        "# Home Assignment 3 (30pts)\n",
        "\n",
        "Submit your solution via Ilias until 23.59h on Tuesday, November 18th. Late submissions are **not possible**.\n",
        "\n",
        "Submit your solutions in teams of 5 students. Unless explicitly agreed otherwise in advance, submissions from teams with more or less members will NOT be graded (i.e., count as failed).\n",
        "\n",
        "**Make sure that all team members are part of the submitting group on Ilias.**\n",
        "\n",
        "You may use the code from the exercises and basic functionalities that are explained in the official documentation of Python packages without citing, __all other sources must be cited__. In case of plagiarism (copying solutions from other teams or from the internet) ALL team members may be expelled from the course without warning.\n",
        "\n",
        "#### General guidelines:\n",
        "* Make sure that your code is executable, any task for which the code does not directly run on our machine will be graded with 0 points.\n",
        "* If you use packages that are not available on the default or conda-forge channel, list them below. Also add a link to installation instructions. \n",
        "* Ensure that the notebook does not rely on the current notebook or system state!\n",
        "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
        "    are not in scope anymore.\n",
        "  * Do not rename any of the datasets you use, and load it from the same directory that your ipynb-notebook is located in, i.e., your working directory.\n",
        "* Make sure you clean up your code before submission, e.g., properly align your code, and delete every line of code that you do not need anymore, even if you may have experimented with it. Minimize usage of global variables. Avoid reusing variable names multiple times!\n",
        "* Ensure your code/notebook terminates in reasonable time.\n",
        "* Feel free to use comments in the code. While we do not require them to get full marks, they may help us in case your code has minor errors.\n",
        "* For questions that require a textual answer, please do not write the answer as a comment in a code cell, but in a Markdown cell below the code. Always remember to provide sufficient justification for all answers.\n",
        "* You may create as many additional cells as you want, just make sure that the solutions to the individual tasks can be found near the corresponding assignment.\n",
        "* If you have any general question regarding the understanding of some task, do not hesitate to post in the student forum in Ilias, so we can clear up such questions for all students in the course."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ed9164",
      "metadata": {},
      "source": [
        "Additional packages (if any):\n",
        " - Example: `powerlaw`, https://github.com/jeffalstott/powerlaw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36ef36f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/I569362/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/I569362/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/I569362/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import List, Union, Dict, Set, Tuple\n",
        "from numpy.typing import NDArray\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4acc6b92",
      "metadata": {},
      "source": [
        "### Task 1: POS tagging (6 points)\n",
        "\n",
        "In this task, we want to explore sentences with similar part of speech (POS) tag structure. For this, we need a corpus of text with tags. We will generate such a corpus by using NLTKâ€™s currently recommended POS tagger to tag a given list of tokens (https://www.nltk.org/api/nltk.tag.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e78bb0d",
      "metadata": {},
      "source": [
        "__a)__ Given a corpus of text ``corpus`` as a sequence of tokens, we want to collect all words that are tagged with a certain POS tag. Implement a function ``collect_words_for_tag`` that first tags the given corpus using NLTK's off-the-shelf tagger imported in the cell above. Then, for each POS tag, collect all words that were tagged with it. You should return a dictionary that maps each POS tag that was observed to the set of words that were assigned this tag in the given corpus. __(2 pts)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5651149b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus.reader.util import StreamBackedCorpusView \n",
        "from nltk import pos_tag\n",
        "\n",
        "def collect_words_for_tag(corpus: Union[List[str], StreamBackedCorpusView]) -> Dict[str, Set[str]]:\n",
        "    \"\"\"\n",
        "    :param corpus: sequence of tokens that represents the text corpus\n",
        "    :return: dict that maps each tag to a set of tokens that were assigned this tag in the corpus\n",
        "    \"\"\"\n",
        "    tagged_corpus = pos_tag(corpus)\n",
        "    pos_dict = defaultdict(set)\n",
        "    for word, tag in tagged_corpus:\n",
        "        pos_dict[tag].add(word)\n",
        "    return pos_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba737321",
      "metadata": {},
      "source": [
        "__b)__ Implement a function ``generate_sentences`` that gets a sentence and a POS dictionary (assume the POS dictionary was generated by your function in __a)__) as input and generates ``n`` sequences of words with the same tag structure. The words in your generated sequence should be randomly taken from the set of words associated with the current tag. \n",
        "\n",
        "Additionally, the user should have the option to achieve sentences of ``better_quality``. Thus, if ``better_quality=True``, make sure that the tag structure of the output sentences actually matches the tag structure of the input sentence, as the tags may change depending on the context. \n",
        "\n",
        "You can assume that the training corpus is large enough to include all possible POS tags. __(2 pts)__\n",
        "\n",
        "_Hint: consider the_ ``random`` _module_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0b4efad8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_rand(sentence: List[str], pos_dict: Dict[str, Set[str]], n: int, better_quality: bool=False) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    :param sentence: input sentence that sets the tag pattern\n",
        "    :param pos_dict: maps each tag to a list of associated words\n",
        "    :param n: number of sentences that should be generated\n",
        "    :return: List of sentences with the same tag structure as the input sentence\n",
        "    \"\"\"\n",
        "    original_tags = [tag for word, tag in pos_tag(sentence)]\n",
        "    generated_sentences = []\n",
        "    \n",
        "    for _ in range(n):\n",
        "        new_sentence = []\n",
        "        if better_quality:\n",
        "            while True:\n",
        "                new_sentence = [random.choice(list(pos_dict[tag])) for tag in original_tags]\n",
        "                new_tags = [tag for word, tag in pos_tag(new_sentence)]\n",
        "                if new_tags == original_tags:\n",
        "                    break\n",
        "        else:\n",
        "            new_sentence = [random.choice(list(pos_dict[tag])) for tag in original_tags]\n",
        "        generated_sentences.append(new_sentence)\n",
        "        \n",
        "    return generated_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a9b3ba",
      "metadata": {},
      "source": [
        "__c)__ Using the input sentence ``This test is very difficult``, test your implementation to generate 10 sentences based on  \n",
        "\n",
        "* \"Emma\" by Jane Austen\n",
        "\n",
        "* The \"King James Bible\"\n",
        "\n",
        "Store your POS dictionary in ``emma_tags``and ``bible_tags``, respectively. Your generated sentences should be stored in ``emma_sent`` and ``bible_sent``. __(2 pts)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a69ab118",
      "metadata": {},
      "outputs": [],
      "source": [
        "sent = [\"This\", \"test\", \"is\", \"very\", \"difficult\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ad042eec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences from Emma:\n",
            "some dulness mails improperly wet\n",
            "neither disorder works needless formal\n",
            "Another sideboard treats alphabetically necessary\n",
            "every ballroom follows extremely exhausted\n",
            "Any pew denotes glibly gratified\n",
            "an solicitation chuses eyes wine\n",
            "No interior dances sufficiently suppress\n",
            "a depend _is_ Fortunately involved\n",
            "An quaint Yours positively young\n",
            "A transaction lives unconsciously thorough\n",
            "\n",
            "Sentences from King James Bible:\n",
            "Another wealth drieth forgiven pine\n",
            "This evilfavouredness purify ever else\n",
            "Each east enrich asketh taught\n",
            "All bleating saith carefully drink\n",
            "obey enchanter hearkeneth lovely ringstraked\n",
            "that revengeth hadst frankly sheddeth\n",
            "smooth flattereth pillars Nevertheless skin\n",
            "An scabbard confesseth deliciously commodious\n",
            "some murmur nameth scorn cockle\n",
            "broth thirsteth oft woven pleaseth\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Emma by Jane Austen\n",
        "emma_corpus = gutenberg.words(\"austen-emma.txt\")\n",
        "emma_tags = collect_words_for_tag(emma_corpus)\n",
        "emma_sent = generate_rand(sent, emma_tags, 10)\n",
        "\n",
        "# King James Bible\n",
        "bible_corpus = gutenberg.words(\"bible-kjv.txt\")\n",
        "bible_tags = collect_words_for_tag(bible_corpus)\n",
        "bible_sent = generate_rand(sent, bible_tags, 10)\n",
        "\n",
        "print(\"Sentences from Emma:\")\n",
        "for s in emma_sent:\n",
        "    print(\" \".join(s))\n",
        "\n",
        "print(\"\\nSentences from King James Bible:\")\n",
        "for s in bible_sent:\n",
        "    print(\" \".join(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beecad4e",
      "metadata": {},
      "source": [
        "### Task 2: The Viterbi algorithm (11 points)\n",
        "Implement the Viterbi algorithm as introduced in the lecture and the exercise. The input of your function is a sentence that should be tagged, a dictionary with state transition probabilites and a dictionary with word emission probabilities. You may assume that the _transition probabilities_ are complete, i.e. the dictionary includes every combination of states. In contrast, we assume that all combinations of words and POS tags that are not in the dictionary of _emission probabilities_ have an emission probability of 0.\n",
        "\n",
        "The function should return a list of POS tags, s.t. that each tag corresponds to a word of the input sentence. Moreover, return the probability of the sequence of POS tags that you found. \n",
        "\n",
        "You can test your function on the given example that was discussed in the Pen&Paper exercise. For the sentence ``the fans watch the race`` and the provided probabilities, your function should return the POS tag sequence ``['DT', 'N', 'V', 'DT', 'N']`` and a probability of ``9.720000000000002e-06``.\n",
        "\n",
        "Additionally, implement beam search in the viterbi algorithm. The beam size is defined by the parameter `beam`. For example for `beam=2` we only keep the best 2 scores per column in each step and discard the rest. You may use the example from the lecture to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e8319309",
      "metadata": {},
      "outputs": [],
      "source": [
        "# test sentence\n",
        "sentence = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
        "\n",
        "# state transition probabilities (complete)\n",
        "state_trans_prob = {('<s>','DT'):0.8,('<s>','N'):0.2,('<s>','V'):0.0,\n",
        "                    ('DT','DT'):0.0,('DT','N'):0.9,('DT','V'):0.1,\n",
        "                    ('N','DT'):0.0,('N','N'):0.5,('N','V'):0.5,\n",
        "                    ('V','DT'):0.5,('V','N'):0.5,('V','V'):0.0}\n",
        "\n",
        "# word emission probabilities (not complete, all combinations that are not present have probability 0)\n",
        "word_emission_prob = {('the','DT'):0.2, ('fans','N'):0.1,('fans','V'):0.2,('watch','N'):0.3,\n",
        "                      ('watch','V'):0.15,('race','N'):0.1,('race','V'):0.3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aa9e4d09",
      "metadata": {},
      "outputs": [],
      "source": [
        "def Viterbi(sentence: List[str], trans_prob: Dict[Tuple[str,str], float], emiss_prob: Dict[Tuple[str,str], float], beam: int=0) -> (List[str], float):\n",
        "    \"\"\"\n",
        "    :param sentence: sentence that we want to tag\n",
        "    :param trans_prob: dict with state transition probabilities\n",
        "    :param emiss_prob: dict with word emission probabilities\n",
        "    :param beam: beam size for beam search. If 0, dont apply beam search\n",
        "    :returns: \n",
        "        - list with POS tags for each input word\n",
        "        - float that indicates the probability of the tag sequence\n",
        "    \"\"\"\n",
        "    states = list(set([s[1] for s in trans_prob.keys()]))\n",
        "    \n",
        "    viterbi_matrix = [{}]\n",
        "    path = {}\n",
        "\n",
        "    # Initialization step\n",
        "    for state in states:\n",
        "        viterbi_matrix[0][state] = trans_prob.get((\"<s>\", state), 0) * emiss_prob.get((sentence[0], state), 0)\n",
        "        path[state] = [state]\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, len(sentence)):\n",
        "        viterbi_matrix.append({})\n",
        "        new_path = {}\n",
        "\n",
        "        for state in states:\n",
        "            max_prob = 0\n",
        "            max_state = None\n",
        "            for prev_state in states:\n",
        "                prob = viterbi_matrix[t-1].get(prev_state, 0.0) * trans_prob.get((prev_state, state), 0) * emiss_prob.get((sentence[t], state), 0)\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    max_state = prev_state\n",
        "            \n",
        "            viterbi_matrix[t][state] = max_prob\n",
        "            if max_state:\n",
        "                new_path[state] = path[max_state] + [state]\n",
        "\n",
        "        path = new_path\n",
        "        if beam > 0:\n",
        "            top_k = sorted(viterbi_matrix[t].items(), key=lambda item: item[1], reverse=True)[:beam]\n",
        "            viterbi_matrix[t] = dict(top_k)\n",
        "\n",
        "\n",
        "    # Termination step\n",
        "    max_prob = 0\n",
        "    best_path = None\n",
        "    if not viterbi_matrix[-1]: # Handle empty last state\n",
        "        return [], 0.0\n",
        "\n",
        "    last_states = viterbi_matrix[-1].keys()\n",
        "    for state in last_states:\n",
        "        if viterbi_matrix[-1][state] > max_prob:\n",
        "            max_prob = viterbi_matrix[-1][state]\n",
        "            best_path = path[state]\n",
        "            \n",
        "    return best_path, max_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa3d329",
      "metadata": {},
      "source": [
        "### Task 1: Term Frequency - Inverse Document Frequency (13 pts)\n",
        "\n",
        "In this task we want to use the term frequency - inverse document frequency (tf-idf) weighting method to compare documents with each other and to queries. \n",
        "\n",
        "In case you need to tokenize any sentences in the following tasks, please use a tokenizer from NLTK and not the ``string.split`` function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2063c1d",
      "metadata": {},
      "source": [
        "__a)__ To test your implementation throughout this task, you are given an example from the exercise. Start by implementing a function ``process_docs`` that takes the provided dictionary of documents and returns the following data structures. __(4 pts)__\n",
        "\n",
        "- ``word2index``: a dictionary that maps each word that appears in any document to a unique integer identifier starting at 0 \n",
        "- ``doc2index``: a dictionary that maps each document name (here given as the dictionary keys) to a unique integer identifier starting at 0\n",
        "- ``index2doc``: a dictionary that maps each document identifier to the corresponding document name (reverse to ``doc2index``)\n",
        "- ``doc_word_vectors``: a dictionary that maps each document name to a list of word ids that indicate which words appeared in the document in their order of appearance. Words that appear multiple times must also be included multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "351147da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# example from exercise 8\n",
        "d1 = \"cold beer beach\"\n",
        "d2 = \"ice cream beer beer\"\n",
        "d3 = \"beach cold ice cream\"\n",
        "d4 = \"cold beer frozen yogurt frozen beer\"\n",
        "d5 = \"frozen ice ice beer ice cream\"\n",
        "d6 = \"yogurt ice cream ice cream\"\n",
        "\n",
        "docs = {\"d1\": d1, \"d2\": d2, \"d3\": d3, \"d4\": d4, \"d5\": d5, \"d6\": d6}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4e42e138",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def process_docs(docs: Dict[str, str]) -> (Dict[str, int], Dict[str, int], Dict[int, str], Dict[str, List[int]]):\n",
        "    \"\"\"\n",
        "    :params docs: dict that maps each document name to the document content\n",
        "    :returns:\n",
        "        - word2index: dict that maps each word to a unique id\n",
        "        - doc2index: dict that maps each document name to a unique id\n",
        "        - index2doc: dict that maps ids to their associated document name\n",
        "        - doc_word_vectors: dict that maps each document name to a list of word ids that appear in it\n",
        "    \"\"\"\n",
        "    word2index = {}\n",
        "    doc2index = {}\n",
        "    index2doc = {}\n",
        "    doc_word_vectors = {}\n",
        "    \n",
        "    word_id = 0\n",
        "    doc_id = 0\n",
        "    \n",
        "    for doc_name, content in docs.items():\n",
        "        if doc_name not in doc2index:\n",
        "            doc2index[doc_name] = doc_id\n",
        "            index2doc[doc_id] = doc_name\n",
        "            doc_id += 1\n",
        "        \n",
        "        tokens = word_tokenize(content.lower())\n",
        "        word_ids = []\n",
        "        for token in tokens:\n",
        "            if token not in word2index:\n",
        "                word2index[token] = word_id\n",
        "                word_id += 1\n",
        "            word_ids.append(word2index[token])\n",
        "        doc_word_vectors[doc_name] = word_ids\n",
        "        \n",
        "    return word2index, doc2index, index2doc, doc_word_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "40d3253f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The output for the provided example could look like this:\n",
        "\n",
        "# word2index:\n",
        "# {'cold': 0, 'beer': 1, 'beach': 2, 'ice': 3, 'cream': 4, 'frozen': 5, 'yogurt': 6}\n",
        "\n",
        "# doc2index:\n",
        "# {'d1': 0, 'd2': 1, 'd3': 2, 'd4': 3, 'd5': 4, 'd6': 5}\n",
        "\n",
        "# index2doc\n",
        "# {0: 'd1', 1: 'd2', 2: 'd3', 3: 'd4', 4: 'd5', 5: 'd6'}\n",
        "\n",
        "# doc_word_vectors:\n",
        "# {'d1': [0, 1, 2],\n",
        "#  'd2': [3, 4, 1, 1],\n",
        "#  'd3': [2, 0, 3, 4],\n",
        "#  'd4': [0, 1, 5, 6, 5, 1],\n",
        "#  'd5': [5, 3, 3, 1, 3, 4],\n",
        "#  'd6': [6, 3, 4, 3, 4]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8edc1e78",
      "metadata": {},
      "source": [
        "__b)__ Set up a term-document matrix where each column corresponds to a document and each row corresponds to a word that was observed in any of the documents. The row/column indices should correspond to the word/document ids that are set in the input dicts ``word2index`` and ``doc2index``. Count how often each word appears in each document and fill the term document matrix. __(3 pts)__\n",
        "\n",
        "_Example: The word \"beer\" with the word id_ ``1`` _appears two times in the document \"d4\" that has the document id_ ``3``. _Therefore the the entry at position_ ``[1, 3]`` _in the term-document matrix is_ ``2``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b8e81f95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def term_document_matrix(doc_word_v: Dict[str, List[int]], doc2index: Dict[str, int], word2index: Dict[str, int]) -> NDArray[NDArray[float]]:\n",
        "    \"\"\"\n",
        "    :param doc_word_v: dict that maps each document to the list of word ids that appear in it\n",
        "    :param doc2index: dict that maps each document name to a unique id\n",
        "    :param word2index: dict that maps each word to a unique id\n",
        "    :return: term-document matrix (each word is a row, each document is a column) that indicates the count of each word in each document \n",
        "    \"\"\"\n",
        "    num_words = len(word2index)\n",
        "    num_docs = len(doc2index)\n",
        "    td_matrix = np.zeros((num_words, num_docs))\n",
        "    \n",
        "    for doc_name, word_ids in doc_word_v.items():\n",
        "        doc_id = doc2index[doc_name]\n",
        "        for word_id in word_ids:\n",
        "            td_matrix[word_id, doc_id] += 1\n",
        "            \n",
        "    return td_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1006fdc",
      "metadata": {},
      "source": [
        "__c)__ Implement the function ``to_tf_idf_matrix`` that takes a term-document matrix and returns the corresponding term frequency (tf) matrix. If the parameter ``idf`` is set to ``True``, the tf-matrix should further be transformed to a tf-idf matrix (i.e. every entry corresponds to the tf-idf value of the associated word and document). Your implementation should leave the input term-document matrix unchanged. __(3 pts)__\n",
        "\n",
        "Use the following formulas:\n",
        "\n",
        "\\begin{equation}\n",
        "  tf_{t,d} =\n",
        "    \\begin{cases}\n",
        "      1+log_{10}\\text{count}(t,d) & \\text{if count}(t, d) > 0\\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}       \n",
        "\\end{equation}  \n",
        "\n",
        "\\begin{equation}\n",
        "  idf_t = log_{10}(\\frac{N}{df_t})\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "  tf\\text{-}idf_{t,d} = tf_{t,d} \\cdot idf_t\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "152c9a28",
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_tf_idf_matrix(td_matrix: NDArray[NDArray[float]], idf: bool=True) -> NDArray[NDArray[float]]:\n",
        "    \"\"\"\n",
        "    :param td_matrix: term-document matrix \n",
        "    :param idf: computes the tf-idf matrix if True, otherwise computes only the tf matrix\n",
        "    :return: matrix with tf(-idf) values for each word-document pair \n",
        "    \"\"\"\n",
        "    tf_matrix = np.copy(td_matrix)\n",
        "    \n",
        "    # Calculate TF\n",
        "    tf_matrix[tf_matrix > 0] = 1 + np.log10(tf_matrix[tf_matrix > 0])\n",
        "\n",
        "    if not idf:\n",
        "        return tf_matrix\n",
        "\n",
        "    # Calculate IDF\n",
        "    num_docs = td_matrix.shape[1]\n",
        "    df = np.count_nonzero(td_matrix, axis=1)\n",
        "    idf_vector = np.log10(num_docs / df)\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tf_idf_matrix = tf_matrix * idf_vector[:, np.newaxis]\n",
        "    \n",
        "    return tf_idf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22006da3",
      "metadata": {},
      "source": [
        "__d)__ We now want to test the implementation on our running example. First, print the tf-idf for each word of the query ``ice beer`` with respect to each document. Second, find the two most similar documents from ``d1, d2, d3`` according to cosine similarity and print all similarity values.  __(3 pts)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "17a29e2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF for query ice beer:\n",
            "\n",
            "Word: ice\n",
            "  d1: 0.0\n",
            "  d2: 0.17609125905568124\n",
            "  d3: 0.17609125905568124\n",
            "  d4: 0.0\n",
            "  d5: 0.260108141521493\n",
            "  d6: 0.22910001000567795\n",
            "\n",
            "Word: beer\n",
            "  d1: 0.17609125905568124\n",
            "  d2: 0.22910001000567795\n",
            "  d3: 0.0\n",
            "  d4: 0.22910001000567795\n",
            "  d5: 0.17609125905568124\n",
            "  d6: 0.0\n",
            "\n",
            "Cosine similarities between d1, d2, d3:\n",
            "  sim(d1, d2): 0.20173094133460298\n",
            "  sim(d1, d3): 0.8732802004950571\n",
            "  sim(d2, d3): 0.29719757610240577\n"
          ]
        }
      ],
      "source": [
        "# Process docs\n",
        "word2index, doc2index, index2doc, doc_word_vectors = process_docs(docs)\n",
        "td_matrix = term_document_matrix(doc_word_vectors, doc2index, word2index)\n",
        "tf_idf_matrix = to_tf_idf_matrix(td_matrix)\n",
        "\n",
        "# Query\n",
        "query = \"ice beer\"\n",
        "query_tokens = word_tokenize(query.lower())\n",
        "\n",
        "print(\"TF-IDF for query ice beer:\")\n",
        "for token in query_tokens:\n",
        "    if token in word2index:\n",
        "        word_id = word2index[token]\n",
        "        print(f\"\\nWord: {token}\")\n",
        "        for doc_id in range(len(doc2index)):\n",
        "            doc_name = index2doc[doc_id]\n",
        "            print(f\"  {doc_name}: {tf_idf_matrix[word_id, doc_id]}\")\n",
        "\n",
        "# Cosine similarity\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "\n",
        "print(\"\\nCosine similarities between d1, d2, d3:\")\n",
        "doc_ids_to_compare = [doc2index[\"d1\"], doc2index[\"d2\"], doc2index[\"d3\"]]\n",
        "for i in range(len(doc_ids_to_compare)):\n",
        "    for j in range(i + 1, len(doc_ids_to_compare)):\n",
        "        doc1_id = doc_ids_to_compare[i]\n",
        "        doc2_id = doc_ids_to_compare[j]\n",
        "        doc1_name = index2doc[doc1_id]\n",
        "        doc2_name = index2doc[doc2_id]\n",
        "        \n",
        "        sim = cosine_similarity(tf_idf_matrix[:, doc1_id], tf_idf_matrix[:, doc2_id])\n",
        "        print(f\"  sim({doc1_name}, {doc2_name}): {sim}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "text-analytics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
